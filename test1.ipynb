{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacobDowns/glamacles_test/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKynimjYRK2b",
        "outputId": "935656c4-881c-40bb-d229-d8f820242b86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/firedrake/firedrake/src/firedrake/firedrake/interpolation.py:385: FutureWarning: The use of `interpolate` to perform the numerical interpolation is deprecated.\n",
            "This feature will be removed very shortly.\n",
            "\n",
            "Instead, import `interpolate` from the `firedrake.__future__` module to update\n",
            "the interpolation's behaviour to return the symbolic `ufl.Interpolate` object associated\n",
            "with this interpolation.\n",
            "\n",
            "You can then assemble the resulting object to get the interpolated quantity\n",
            "of interest. For example,\n",
            "\n",
            "```\n",
            "from firedrake.__future__ import interpolate\n",
            "...\n",
            "\n",
            "assemble(interpolate(expr, V))\n",
            "```\n",
            "\n",
            "Alternatively, you can also perform other symbolic operations on the interpolation operator, such as taking\n",
            "the derivative, and then assemble the resulting form.\n",
            "\n",
            "  warnings.warn(\"\"\"The use of `interpolate` to perform the numerical interpolation is deprecated.\n",
            "/home/firedrake/firedrake/src/firedrake/firedrake/interpolation.py:385: FutureWarning: The use of `interpolate` to perform the numerical interpolation is deprecated.\n",
            "This feature will be removed very shortly.\n",
            "\n",
            "Instead, import `interpolate` from the `firedrake.__future__` module to update\n",
            "the interpolation's behaviour to return the symbolic `ufl.Interpolate` object associated\n",
            "with this interpolation.\n",
            "\n",
            "You can then assemble the resulting object to get the interpolated quantity\n",
            "of interest. For example,\n",
            "\n",
            "```\n",
            "from firedrake.__future__ import interpolate\n",
            "...\n",
            "\n",
            "assemble(interpolate(expr, V))\n",
            "```\n",
            "\n",
            "Alternatively, you can also perform other symbolic operations on the interpolation operator, such as taking\n",
            "the derivative, and then assemble the resulting form.\n",
            "\n",
            "  warnings.warn(\"\"\"The use of `interpolate` to perform the numerical interpolation is deprecated.\n"
          ]
        }
      ],
      "source": [
        "import firedrake as fd\n",
        "from model import SpecFO\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "model = SpecFO()\n",
        "data = np.load('data/data.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "96y-eTGwhFU_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create input features\n",
        "B = data[:,0,:,:]\n",
        "H = data[:,1,:,:]\n",
        "S = B+H\n",
        "S_x = np.gradient(S, axis=2)\n",
        "S_y = np.gradient(S, axis=1)\n",
        "X = np.stack([H, S_x, S_y], axis=1)\n",
        "\n",
        "# Output features\n",
        "Y = data[:,2:,:,:]\n",
        "\n",
        "# Normalization\n",
        "X /= X.std(axis=(0,2,3))[np.newaxis,:,np.newaxis,np.newaxis]\n",
        "Y /= Y.std(axis=(0,2,3))[np.newaxis,:,np.newaxis,np.newaxis]\n",
        "\n",
        "# Create datsets\n",
        "N_train = 2000\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "Y = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(X[0:N_train], Y[0:N_train])\n",
        "validation_dataset = TensorDataset(X[N_train:], Y[N_train:])\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN7mBgdGw5cb",
        "outputId": "71d6a642-d29a-4097-cc22-f7db442c528a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg_loss 0.1303151687363861\n",
            "validation_loss 0.009572764673549682\n",
            "avg_loss 0.0493670450047357\n",
            "avg_loss 0.04523443771712482\n",
            "avg_loss 0.04259942564630183\n",
            "avg_loss 0.04234569265443133\n",
            "avg_loss 0.03803399472287856\n",
            "avg_loss 0.03601163247518707\n",
            "avg_loss 0.03469529780256562\n",
            "avg_loss 0.031499064227507916\n",
            "avg_loss 0.031643184785963965\n",
            "avg_loss 0.029117395719862542\n",
            "validation_loss 0.0051994941167999055\n",
            "avg_loss 0.0282710539966356\n",
            "avg_loss 0.028204611211607698\n",
            "avg_loss 0.026507551262067865\n",
            "avg_loss 0.025426495507010257\n",
            "avg_loss 0.025717643451702316\n",
            "avg_loss 0.024982524751132586\n",
            "avg_loss 0.024419724831241184\n",
            "avg_loss 0.022871167279750807\n",
            "avg_loss 0.021612393893505215\n",
            "avg_loss 0.0222585211553087\n",
            "validation_loss 0.003259282735432498\n",
            "avg_loss 0.021710637648386183\n",
            "avg_loss 0.020549417398055082\n",
            "avg_loss 0.01981383199649281\n",
            "avg_loss 0.02117550425507943\n",
            "avg_loss 0.0196045850829687\n",
            "avg_loss 0.019336133171775145\n",
            "avg_loss 0.018947884078137578\n",
            "avg_loss 0.0187406107953866\n",
            "avg_loss 0.01935881278567831\n",
            "avg_loss 0.018770825343177422\n",
            "validation_loss 0.0031316746160387995\n",
            "avg_loss 0.018392562536813786\n",
            "avg_loss 0.016887234821595484\n",
            "avg_loss 0.016851407785725313\n",
            "avg_loss 0.016431906117824836\n",
            "avg_loss 0.016108647913773894\n",
            "avg_loss 0.016104922422018716\n",
            "avg_loss 0.01587516198110825\n",
            "avg_loss 0.016047881840597255\n",
            "avg_loss 0.015629505169927144\n",
            "avg_loss 0.014807744413614272\n",
            "validation_loss 0.0037147541989572346\n",
            "avg_loss 0.015371974116322235\n",
            "avg_loss 0.014790544508621679\n",
            "avg_loss 0.0167360048857372\n",
            "avg_loss 0.014353369139353162\n",
            "avg_loss 0.014317263351462316\n",
            "avg_loss 0.013006805757366237\n",
            "avg_loss 0.01449395017597999\n",
            "avg_loss 0.013756969800815567\n",
            "avg_loss 0.015158433429562138\n",
            "avg_loss 0.013697352197850706\n",
            "validation_loss 0.0033681847023777664\n",
            "avg_loss 0.013653148540150142\n",
            "avg_loss 0.0131367379292642\n",
            "avg_loss 0.013225677023685422\n",
            "avg_loss 0.014637536100344732\n",
            "avg_loss 0.014048469541332452\n",
            "avg_loss 0.013356017303725822\n",
            "avg_loss 0.013106404594669585\n",
            "avg_loss 0.012392275020072702\n",
            "avg_loss 0.013548129050061107\n",
            "avg_loss 0.013014475935473457\n",
            "validation_loss 0.007679230898618698\n",
            "avg_loss 0.013687088741440675\n",
            "avg_loss 0.012417978971498086\n",
            "avg_loss 0.01180430777442234\n",
            "avg_loss 0.012896908388327574\n",
            "avg_loss 0.013762155978241935\n",
            "avg_loss 0.011575450866788742\n",
            "avg_loss 0.011600325138075277\n",
            "avg_loss 0.012448186471039662\n",
            "avg_loss 0.013535591117863078\n",
            "avg_loss 0.01296138096343202\n",
            "validation_loss 0.003568409300316125\n",
            "avg_loss 0.011733707758583478\n",
            "avg_loss 0.011020731706215883\n",
            "avg_loss 0.011380981602749671\n",
            "avg_loss 0.010872596724017058\n",
            "avg_loss 0.012100205799375544\n",
            "avg_loss 0.01182476471310656\n",
            "avg_loss 0.012517223222559552\n",
            "avg_loss 0.011947302290602238\n",
            "avg_loss 0.010856522708229023\n",
            "avg_loss 0.012072616080011358\n",
            "validation_loss 0.002034188058809377\n",
            "avg_loss 0.012080851757680647\n",
            "avg_loss 0.011598211196964258\n",
            "avg_loss 0.011597880755987717\n",
            "avg_loss 0.011371960647884406\n",
            "avg_loss 0.01080035397654865\n",
            "avg_loss 0.010560440478526289\n",
            "avg_loss 0.012819667862975621\n",
            "avg_loss 0.011482688764226623\n",
            "avg_loss 0.011608389274697402\n",
            "avg_loss 0.009536896323566907\n",
            "validation_loss 0.002144198239885736\n",
            "avg_loss 0.010514996567551862\n",
            "avg_loss 0.011356194312305888\n",
            "avg_loss 0.010761277002544376\n",
            "avg_loss 0.011123049049187102\n",
            "avg_loss 0.01078627875483653\n",
            "avg_loss 0.011694117302089581\n",
            "avg_loss 0.010885528700280702\n",
            "avg_loss 0.011273371024930383\n",
            "avg_loss 0.011199349374161102\n"
          ]
        }
      ],
      "source": [
        "from modulus.models.fno import FNO\n",
        "\n",
        "# Fourier neural operator model\n",
        "fno = FNO(\n",
        "    in_channels=3,\n",
        "    out_channels=4,\n",
        "    decoder_layers=1,\n",
        "    decoder_layer_size=32,\n",
        "    dimension=2,\n",
        "    latent_channels=32,\n",
        "    num_fno_layers=5,\n",
        "    num_fno_modes=16,\n",
        "    padding=9,\n",
        ").cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(fno.parameters(), lr=1e-4)\n",
        "\n",
        "# Number of epochs\n",
        "epochs = 100\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    avg_loss = 0.\n",
        "    num = 0\n",
        "\n",
        "    fno.train()\n",
        "    for i, (x,y) in enumerate(train_loader):\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        y_fno = fno(x)\n",
        "        loss = torch.mean(torch.sqrt((y - y_fno)**2 + 1e-10))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss += loss.item()\n",
        "        num += 1\n",
        "\n",
        "    print('avg_loss', avg_loss / num)\n",
        "\n",
        "    # Validation \n",
        "    if epoch % 10 == 0:\n",
        "        with torch.no_grad():\n",
        "            validation_loss = 0.\n",
        "            for i, (x,y) in enumerate(validation_loader):\n",
        "                x = x.cuda()\n",
        "                y = y.cuda()\n",
        "                y_fno = fno(x)\n",
        "                loss = torch.mean(torch.sqrt((y - y_fno)**2 + 1e-10))\n",
        "                validation_loss += loss.item()\n",
        "                num += 1\n",
        "            print('validation_loss', validation_loss / num)\n",
        "   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUAz48276xHL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
